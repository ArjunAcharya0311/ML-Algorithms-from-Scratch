{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.00632</th>\n",
       "      <th>18</th>\n",
       "      <th>2.31</th>\n",
       "      <th>0</th>\n",
       "      <th>0.538</th>\n",
       "      <th>6.575</th>\n",
       "      <th>65.2</th>\n",
       "      <th>4.09</th>\n",
       "      <th>1</th>\n",
       "      <th>296</th>\n",
       "      <th>15.3</th>\n",
       "      <th>396.9</th>\n",
       "      <th>4.98</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "      <td>28.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0.00632   18  2.31  0  0.538  6.575  65.2    4.09  1  296  15.3   396.9  \\\n",
       "0  0.02731  0.0  7.07  0  0.469  6.421  78.9  4.9671  2  242  17.8  396.90   \n",
       "1  0.02729  0.0  7.07  0  0.469  7.185  61.1  4.9671  2  242  17.8  392.83   \n",
       "2  0.03237  0.0  2.18  0  0.458  6.998  45.8  6.0622  3  222  18.7  394.63   \n",
       "3  0.06905  0.0  2.18  0  0.458  7.147  54.2  6.0622  3  222  18.7  396.90   \n",
       "4  0.02985  0.0  2.18  0  0.458  6.430  58.7  6.0622  3  222  18.7  394.12   \n",
       "\n",
       "   4.98    24  \n",
       "0  9.14  21.6  \n",
       "1  4.03  34.7  \n",
       "2  2.94  33.4  \n",
       "3  5.33  36.2  \n",
       "4  5.21  28.7  "
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = pd.read_csv(\"housing.csv\")\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.00632</th>\n",
       "      <th>18</th>\n",
       "      <th>2.31</th>\n",
       "      <th>0</th>\n",
       "      <th>0.538</th>\n",
       "      <th>6.575</th>\n",
       "      <th>65.2</th>\n",
       "      <th>4.09</th>\n",
       "      <th>1</th>\n",
       "      <th>296</th>\n",
       "      <th>15.3</th>\n",
       "      <th>396.9</th>\n",
       "      <th>4.98</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.00632</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.200283</td>\n",
       "      <td>0.406251</td>\n",
       "      <td>-0.056132</td>\n",
       "      <td>0.420934</td>\n",
       "      <td>-0.218978</td>\n",
       "      <td>0.352701</td>\n",
       "      <td>-0.379626</td>\n",
       "      <td>0.625395</td>\n",
       "      <td>0.582568</td>\n",
       "      <td>0.289393</td>\n",
       "      <td>-0.384838</td>\n",
       "      <td>0.455328</td>\n",
       "      <td>-0.388249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.200283</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.534022</td>\n",
       "      <td>-0.042550</td>\n",
       "      <td>-0.516574</td>\n",
       "      <td>0.311835</td>\n",
       "      <td>-0.569524</td>\n",
       "      <td>0.664396</td>\n",
       "      <td>-0.311717</td>\n",
       "      <td>-0.314351</td>\n",
       "      <td>-0.391713</td>\n",
       "      <td>0.175319</td>\n",
       "      <td>-0.412894</td>\n",
       "      <td>0.360393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.31</th>\n",
       "      <td>0.406251</td>\n",
       "      <td>-0.534022</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.062350</td>\n",
       "      <td>0.764556</td>\n",
       "      <td>-0.391330</td>\n",
       "      <td>0.645543</td>\n",
       "      <td>-0.708848</td>\n",
       "      <td>0.594167</td>\n",
       "      <td>0.720561</td>\n",
       "      <td>0.380955</td>\n",
       "      <td>-0.356506</td>\n",
       "      <td>0.602737</td>\n",
       "      <td>-0.484126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.056132</td>\n",
       "      <td>-0.042550</td>\n",
       "      <td>0.062350</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.091134</td>\n",
       "      <td>0.091497</td>\n",
       "      <td>0.086461</td>\n",
       "      <td>-0.099109</td>\n",
       "      <td>-0.007907</td>\n",
       "      <td>-0.035965</td>\n",
       "      <td>-0.122570</td>\n",
       "      <td>0.049040</td>\n",
       "      <td>-0.054576</td>\n",
       "      <td>0.175364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.538</th>\n",
       "      <td>0.420934</td>\n",
       "      <td>-0.516574</td>\n",
       "      <td>0.764556</td>\n",
       "      <td>0.091134</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.302127</td>\n",
       "      <td>0.731461</td>\n",
       "      <td>-0.769220</td>\n",
       "      <td>0.611758</td>\n",
       "      <td>0.668141</td>\n",
       "      <td>0.188918</td>\n",
       "      <td>-0.380006</td>\n",
       "      <td>0.591262</td>\n",
       "      <td>-0.427295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.575</th>\n",
       "      <td>-0.218978</td>\n",
       "      <td>0.311835</td>\n",
       "      <td>-0.391330</td>\n",
       "      <td>0.091497</td>\n",
       "      <td>-0.302127</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.240211</td>\n",
       "      <td>0.205170</td>\n",
       "      <td>-0.209277</td>\n",
       "      <td>-0.291680</td>\n",
       "      <td>-0.355116</td>\n",
       "      <td>0.127754</td>\n",
       "      <td>-0.613734</td>\n",
       "      <td>0.695365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65.2</th>\n",
       "      <td>0.352701</td>\n",
       "      <td>-0.569524</td>\n",
       "      <td>0.645543</td>\n",
       "      <td>0.086461</td>\n",
       "      <td>0.731461</td>\n",
       "      <td>-0.240211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.747872</td>\n",
       "      <td>0.456232</td>\n",
       "      <td>0.506527</td>\n",
       "      <td>0.261724</td>\n",
       "      <td>-0.273486</td>\n",
       "      <td>0.602782</td>\n",
       "      <td>-0.376932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.09</th>\n",
       "      <td>-0.379626</td>\n",
       "      <td>0.664396</td>\n",
       "      <td>-0.708848</td>\n",
       "      <td>-0.099109</td>\n",
       "      <td>-0.769220</td>\n",
       "      <td>0.205170</td>\n",
       "      <td>-0.747872</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.494797</td>\n",
       "      <td>-0.534492</td>\n",
       "      <td>-0.232560</td>\n",
       "      <td>0.291451</td>\n",
       "      <td>-0.497276</td>\n",
       "      <td>0.249895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.625395</td>\n",
       "      <td>-0.311717</td>\n",
       "      <td>0.594167</td>\n",
       "      <td>-0.007907</td>\n",
       "      <td>0.611758</td>\n",
       "      <td>-0.209277</td>\n",
       "      <td>0.456232</td>\n",
       "      <td>-0.494797</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.910202</td>\n",
       "      <td>0.463322</td>\n",
       "      <td>-0.444065</td>\n",
       "      <td>0.487608</td>\n",
       "      <td>-0.381690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.582568</td>\n",
       "      <td>-0.314351</td>\n",
       "      <td>0.720561</td>\n",
       "      <td>-0.035965</td>\n",
       "      <td>0.668141</td>\n",
       "      <td>-0.291680</td>\n",
       "      <td>0.506527</td>\n",
       "      <td>-0.534492</td>\n",
       "      <td>0.910202</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.460100</td>\n",
       "      <td>-0.441505</td>\n",
       "      <td>0.543435</td>\n",
       "      <td>-0.468543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15.3</th>\n",
       "      <td>0.289393</td>\n",
       "      <td>-0.391713</td>\n",
       "      <td>0.380955</td>\n",
       "      <td>-0.122570</td>\n",
       "      <td>0.188918</td>\n",
       "      <td>-0.355116</td>\n",
       "      <td>0.261724</td>\n",
       "      <td>-0.232560</td>\n",
       "      <td>0.463322</td>\n",
       "      <td>0.460100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.176515</td>\n",
       "      <td>0.372148</td>\n",
       "      <td>-0.508411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396.9</th>\n",
       "      <td>-0.384838</td>\n",
       "      <td>0.175319</td>\n",
       "      <td>-0.356506</td>\n",
       "      <td>0.049040</td>\n",
       "      <td>-0.380006</td>\n",
       "      <td>0.127754</td>\n",
       "      <td>-0.273486</td>\n",
       "      <td>0.291451</td>\n",
       "      <td>-0.444065</td>\n",
       "      <td>-0.441505</td>\n",
       "      <td>-0.176515</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.365637</td>\n",
       "      <td>0.333394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.98</th>\n",
       "      <td>0.455328</td>\n",
       "      <td>-0.412894</td>\n",
       "      <td>0.602737</td>\n",
       "      <td>-0.054576</td>\n",
       "      <td>0.591262</td>\n",
       "      <td>-0.613734</td>\n",
       "      <td>0.602782</td>\n",
       "      <td>-0.497276</td>\n",
       "      <td>0.487608</td>\n",
       "      <td>0.543435</td>\n",
       "      <td>0.372148</td>\n",
       "      <td>-0.365637</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.738187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.388249</td>\n",
       "      <td>0.360393</td>\n",
       "      <td>-0.484126</td>\n",
       "      <td>0.175364</td>\n",
       "      <td>-0.427295</td>\n",
       "      <td>0.695365</td>\n",
       "      <td>-0.376932</td>\n",
       "      <td>0.249895</td>\n",
       "      <td>-0.381690</td>\n",
       "      <td>-0.468543</td>\n",
       "      <td>-0.508411</td>\n",
       "      <td>0.333394</td>\n",
       "      <td>-0.738187</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0.00632        18      2.31         0     0.538     6.575      65.2  \\\n",
       "0.00632  1.000000 -0.200283  0.406251 -0.056132  0.420934 -0.218978  0.352701   \n",
       "18      -0.200283  1.000000 -0.534022 -0.042550 -0.516574  0.311835 -0.569524   \n",
       "2.31     0.406251 -0.534022  1.000000  0.062350  0.764556 -0.391330  0.645543   \n",
       "0       -0.056132 -0.042550  0.062350  1.000000  0.091134  0.091497  0.086461   \n",
       "0.538    0.420934 -0.516574  0.764556  0.091134  1.000000 -0.302127  0.731461   \n",
       "6.575   -0.218978  0.311835 -0.391330  0.091497 -0.302127  1.000000 -0.240211   \n",
       "65.2     0.352701 -0.569524  0.645543  0.086461  0.731461 -0.240211  1.000000   \n",
       "4.09    -0.379626  0.664396 -0.708848 -0.099109 -0.769220  0.205170 -0.747872   \n",
       "1        0.625395 -0.311717  0.594167 -0.007907  0.611758 -0.209277  0.456232   \n",
       "296      0.582568 -0.314351  0.720561 -0.035965  0.668141 -0.291680  0.506527   \n",
       "15.3     0.289393 -0.391713  0.380955 -0.122570  0.188918 -0.355116  0.261724   \n",
       "396.9   -0.384838  0.175319 -0.356506  0.049040 -0.380006  0.127754 -0.273486   \n",
       "4.98     0.455328 -0.412894  0.602737 -0.054576  0.591262 -0.613734  0.602782   \n",
       "24      -0.388249  0.360393 -0.484126  0.175364 -0.427295  0.695365 -0.376932   \n",
       "\n",
       "             4.09         1       296      15.3     396.9      4.98        24  \n",
       "0.00632 -0.379626  0.625395  0.582568  0.289393 -0.384838  0.455328 -0.388249  \n",
       "18       0.664396 -0.311717 -0.314351 -0.391713  0.175319 -0.412894  0.360393  \n",
       "2.31    -0.708848  0.594167  0.720561  0.380955 -0.356506  0.602737 -0.484126  \n",
       "0       -0.099109 -0.007907 -0.035965 -0.122570  0.049040 -0.054576  0.175364  \n",
       "0.538   -0.769220  0.611758  0.668141  0.188918 -0.380006  0.591262 -0.427295  \n",
       "6.575    0.205170 -0.209277 -0.291680 -0.355116  0.127754 -0.613734  0.695365  \n",
       "65.2    -0.747872  0.456232  0.506527  0.261724 -0.273486  0.602782 -0.376932  \n",
       "4.09     1.000000 -0.494797 -0.534492 -0.232560  0.291451 -0.497276  0.249895  \n",
       "1       -0.494797  1.000000  0.910202  0.463322 -0.444065  0.487608 -0.381690  \n",
       "296     -0.534492  0.910202  1.000000  0.460100 -0.441505  0.543435 -0.468543  \n",
       "15.3    -0.232560  0.463322  0.460100  1.000000 -0.176515  0.372148 -0.508411  \n",
       "396.9    0.291451 -0.444065 -0.441505 -0.176515  1.000000 -0.365637  0.333394  \n",
       "4.98    -0.497276  0.487608  0.543435  0.372148 -0.365637  1.000000 -0.738187  \n",
       "24       0.249895 -0.381690 -0.468543 -0.508411  0.333394 -0.738187  1.000000  "
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, X, y, learningrate, tolerance, maxIteration = 50000, error = 'rmse', gd = False, \n",
    "                regression = False, stochastic = False, batch_size = 100, regularize = False, regLambda = 1):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.learningrate = learningrate\n",
    "        self.tolerance = tolerance\n",
    "        self.maxIteration = maxIteration\n",
    "        self.error = error\n",
    "        self.gd = gd\n",
    "        self.regression = regression\n",
    "        self.stochastic = stochastic\n",
    "        self.batch_size = batch_size\n",
    "        self.regularize = regularize\n",
    "        self.regLambda = regLambda\n",
    "    \n",
    "    # divide data into training and testing samples\n",
    "    def trainTestSplit(self, X, y):\n",
    "        \"\"\"\n",
    "        @X: numpy matrix, dataset\n",
    "        @y: numpy array, target value\n",
    "        @does: splitting dataset to train and test \n",
    "        @return: numpy matrix, matrix, array, array\n",
    "        \"\"\"\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "    \n",
    "    def addX0(self, X):\n",
    "        \"\"\"\n",
    "        @X: numpy matrix, dataset\n",
    "        @does: add a bias term to the data\n",
    "        @return: numpy matrix\n",
    "        \"\"\"\n",
    "        return np.column_stack([np.ones([X.shape[0], 1]), X])\n",
    "    \n",
    "    def normalize(self, X):\n",
    "        \"\"\"\n",
    "        @X: numpy matrix, dataset\n",
    "        @does: normalize the dataset\n",
    "        @return: numpy matrix, array, array\n",
    "        \"\"\"\n",
    "        \n",
    "        mean = np.mean(X, 0) # mean of each column defined by the 0\n",
    "        std = np.std(X,0)\n",
    "        X_norm = (X - mean)/std\n",
    "        X_norm = self.addX0(X_norm)\n",
    "        \n",
    "        return X_norm, mean, std # we will be normalizing the test data based on the mean and std of training data\n",
    "    \n",
    "    def normalizeTestData(self, X, trainMean, trainStd):\n",
    "        \"\"\"\n",
    "        @X: numpy matrix, dataset\n",
    "        @does: normalize the test dataset\n",
    "        @return: numpy matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        X_norm = (X - trainMean)/trainStd\n",
    "        X_norm = self.addX0(X_norm)\n",
    "        \n",
    "        return X_norm\n",
    "    \n",
    "    def rank(self, X, eps = 1e-12):\n",
    "        \"\"\"\n",
    "        @X: numpy matrix\n",
    "        @eps: float\n",
    "        @does: return rank of the matrix\n",
    "        @return: numpy matrix\n",
    "        \"\"\"\n",
    "        u, S, vh = np.linalg.svd(X)\n",
    "        return len([x for x in S if abs(x) > eps])\n",
    "    \n",
    "    def checkMatrix(self, X):\n",
    "        \"\"\"\n",
    "        @X: numpy matrix\n",
    "        @does: check if the matrix is full rank\n",
    "        @return\n",
    "        \"\"\"\n",
    "        x_rank = self.rank(X)\n",
    "        if x_rank == min(X.shape[0], X.shape[1]):\n",
    "            self.fullRank = True\n",
    "            print(\"Matrix is full rank\")\n",
    "        else:\n",
    "            self.fullRank = False\n",
    "            print(\"Matrix is not full rank\")\n",
    "            \n",
    "    def checkInvertibility(self, X):\n",
    "        \"\"\"\n",
    "        @X: numpy matrix\n",
    "        @does: check if the matrix is low rank\n",
    "        @return\n",
    "        \"\"\"\n",
    "        if X.shape[0] < X.shape[1]:\n",
    "            self.lowRank = True\n",
    "            print('The matrix is low rank')\n",
    "        else:\n",
    "            self.lowRank = False\n",
    "            \n",
    "    def closedFormSolution(self, X, y):\n",
    "        \"\"\"\n",
    "        @X: numpy matrix, dataset\n",
    "        @y: numpy array, target value\n",
    "        @does: solve regression using closed form solution \n",
    "        @return: numpy array - parameters theta\n",
    "        \"\"\"\n",
    "        return np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        @X: numpy matrix, dataset\n",
    "        @does: compute prediction \n",
    "        @return: numpy array - prediction array\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "    \n",
    "    def sse(self, X, y): #standard square error\n",
    "        \"\"\"\n",
    "        @X: numpy matrix, dataset\n",
    "        @y: numpy array, target value\n",
    "        @does: compute sum of squared error \n",
    "        @return: integer - overall SSE\n",
    "        \"\"\"\n",
    "        y_hat = self.predict(X)\n",
    "        return ((y_hat - y)**2).sum()\n",
    "    \n",
    "    def rmse(self, X, y):\n",
    "        \"\"\"\n",
    "        @X: numpy matrix, dataset\n",
    "        @y: numpy array, target value\n",
    "        @does: compute root mean square error \n",
    "        @return: integer - overall RMSE\n",
    "        \"\"\"\n",
    "        return math.sqrt(self.sse(X,y) / y.size)\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        \"\"\"\n",
    "        @X: numpy matrix, dataset\n",
    "        @y: numpy array, target value\n",
    "        @does: compute root mean square error \n",
    "        @return: integer - overall cost\n",
    "        \"\"\"\n",
    "        return self.sse(X,y)/2\n",
    "    \n",
    "    def costDerivative(self, X, y):\n",
    "        \"\"\"\n",
    "        @X: numpy matrix, dataset\n",
    "        @y: numpy array, target value\n",
    "        @does: computes derivative of the cost function \n",
    "        @return: integer - derivative value\n",
    "        \"\"\"\n",
    "        y_hat = self.predict(X)\n",
    "        return (y_hat - y).dot(X) #2X.T*Theta*X - 2X.T*y\n",
    "\n",
    "    def gradientDescent(self, X, y):\n",
    "        \"\"\"\n",
    "        @X: numpy matrix, dataset\n",
    "        @y: numpy array, target value\n",
    "        @does: normal gradient descent and stochastic gradient descent\n",
    "        @return\n",
    "        \"\"\"\n",
    "        error_sequences = []\n",
    "        previous_error = float('inf')\n",
    "        \n",
    "        # Stochastic Gradient Descent\n",
    "        \n",
    "        if self.stochastic:\n",
    "            print(\"Stochastic Gradient Descent\")\n",
    "            for i in tqdm(range(self.maxIteration)):\n",
    "                idx = np.random.randint(0, X.shape[0], self.batch_size) #randomly generate indexes\n",
    "                X_batch = X[idx, :] #randomly sample from X\n",
    "                y_batch = y[idx] #randomly sample from y\n",
    "                self.w = self.w - self.learningrate * self.costDerivative(X_batch, y_batch)\n",
    "                if self.error == 'rmse':\n",
    "                    current_error = self.rmse(X, y)\n",
    "                else:\n",
    "                    current_error = self.sse(X, y)\n",
    "                diff = previous_error - current_error\n",
    "                previous_error = current_error\n",
    "                if diff < self.tolerance:\n",
    "                    print(\"No further imporvements\")\n",
    "                    break\n",
    "                    \n",
    "        else:\n",
    "            print(\"Normal Gradient Descent\")\n",
    "            for i in tqdm(range(self.maxIteration)):\n",
    "                self.w = self.w - self.learningrate * self.costDerivative(X, y)\n",
    "                if self.error == 'rmse':\n",
    "                    current_error = self.rmse(X, y)\n",
    "                else:\n",
    "                    current_error = self.sse(X, y)\n",
    "                    \n",
    "                diff = previous_error - current_error\n",
    "                previous_error = current_error\n",
    "                if diff < self.tolerance:\n",
    "                    print(\"No further imporvement\")\n",
    "                    break\n",
    "        return\n",
    "    \n",
    "    \n",
    "    # L2 Regularization \n",
    "    \n",
    "    def regCostDerivative(self, X, y):\n",
    "        \"\"\"\n",
    "        @X: numpy matrix, dataset\n",
    "        @y: numpy array, target value\n",
    "        @does: computes cost derivative for L2 regularization \n",
    "        @return: integer - overall cost\n",
    "        \"\"\"\n",
    "        return self.costDerivative(X, y) + self.regLambda * self.w\n",
    "     \n",
    "    def RidgeRegularization(self, X, y):\n",
    "        \"\"\"\n",
    "        @X: numpy matrix, dataset\n",
    "        @y: numpy array, target value\n",
    "        @does: computes w for Ridge Regularization using both closed form and grdient descent\n",
    "        @return: integer - overall cost\n",
    "        \"\"\"\n",
    "        if self.gd:\n",
    "            print(\"Regularization - Gradient Descent\")\n",
    "            previous_error = float('inf')\n",
    "            for i in tqdm(range(self.maxIteration)):\n",
    "                self.w = self.w - self.learningrate * self.regCostDerivative(X, y)\n",
    "                if self.error == 'rmse':\n",
    "                    current_error = self.rmse(X, y)\n",
    "                else:\n",
    "                    current_error = self.sse(X, y)\n",
    "                    \n",
    "                diff = previous_error - current_error\n",
    "                previous_error = current_error\n",
    "                if diff < self.tolerance:\n",
    "                    print(\"No further imporvement\")\n",
    "                    break\n",
    "        else:\n",
    "            print(\"Regularization - Closed Form Solution\")\n",
    "            I = np.identity(X.shape[1])\n",
    "            self.w = np.linalg.inv(X.T.dot(X) + self.regLambda*I).dot(X.T).dot(y)\n",
    "        \n",
    "    \n",
    "    def trainModel(self):\n",
    "        \"\"\"\n",
    "        @does: combines above methods to train model\n",
    "        \"\"\"\n",
    "        # Split dataset to train and test\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = self.trainTestSplit(self.X, self.y)\n",
    "        \n",
    "        # Normalize Data\n",
    "        self.X_train, self.mean, self.std = self.normalize(self.X_train)\n",
    "        self.X_test = self.normalizeTestData(self.X_test, self.mean, self.std)\n",
    "        \n",
    "        # Check matrix rank\n",
    "        self.checkMatrix(self.X_train)\n",
    "        self.checkInvertibility(self.X_train)\n",
    "        \n",
    "        # No Regularization\n",
    "        if not self.regularize:\n",
    "            # Closed Form Solution\n",
    "            if self.fullRank and not self.lowRank and self.X_train.shape[0] < 10000 and not self.gd:\n",
    "                print(\"Normal Closed Form Solution\")\n",
    "                self.w = self.closedFormSolution(self.X_train, self.y_train)\n",
    "            \n",
    "            # Gradient Descent\n",
    "            else:\n",
    "                self.w = np.ones(self.X_train.shape[1], dtype = np.float64) * 0\n",
    "                self.gradientDescent(self.X_train, self.y_train)\n",
    "        \n",
    "        # With Regularization\n",
    "        else:\n",
    "            self.w = np.ones(self.X_train.shape[1], dtype = np.float64) * 0\n",
    "            self.RidgeRegularization(self.X_train, self.y_train)\n",
    "            \n",
    "            \n",
    "        print(self.w)\n",
    "        \n",
    "        if self.error == 'rmse':\n",
    "            print(self.rmse(self.X_test, self.y_test))\n",
    "        else:\n",
    "            print(self.sse(self.X_test, self.y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression  = LinearRegression(housing.values[:, 0:-1], housing.values[:,-1], \n",
    "                              learningrate = 0.000001,\n",
    "                              tolerance = 0.0000001,\n",
    "                              gd = True,\n",
    "                              error = 'rmse',\n",
    "                              stochastic = False,\n",
    "                              batch_size = 150,\n",
    "                              regularize = True,\n",
    "                              regLambda = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▍                                                                       | 1627/50000 [00:00<00:02, 16170.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix is full rank\n",
      "Regularization - Gradient Descent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 50000/50000 [00:02<00:00, 16708.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.86920857 -0.85100296  0.93948444  0.14432545  0.69565732 -1.96577407\n",
      "  2.67322031  0.3778522  -2.53847105  1.65766706 -1.01910805 -2.27888403\n",
      "  0.65303712 -3.92456451]\n",
      "4.638097616704881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "regression.trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>540</th>\n",
       "      <th>0</th>\n",
       "      <th>0.1</th>\n",
       "      <th>162</th>\n",
       "      <th>2.5</th>\n",
       "      <th>1040</th>\n",
       "      <th>676</th>\n",
       "      <th>28</th>\n",
       "      <th>79.99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>266.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>90</td>\n",
       "      <td>47.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     540      0  0.1    162  2.5    1040    676   28  79.99\n",
       "0  540.0    0.0  0.0  162.0  2.5  1055.0  676.0   28  61.89\n",
       "1  332.5  142.5  0.0  228.0  0.0   932.0  594.0  270  40.27\n",
       "2  332.5  142.5  0.0  228.0  0.0   932.0  594.0  365  41.05\n",
       "3  198.6  132.4  0.0  192.0  0.0   978.4  825.5  360  44.30\n",
       "4  266.0  114.0  0.0  228.0  0.0   932.0  670.0   90  47.03"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete = pd.read_csv(\"concreteData.csv\")\n",
    "concrete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_concrete  = LinearRegression(concrete.values[:, 0:-1], concrete.values[:,-1], \n",
    "                              learningrate = 0.000001,\n",
    "                              tolerance = 0.0000001,\n",
    "                              gd = True,\n",
    "                              error = 'rmse',\n",
    "                              stochastic = False,\n",
    "                              batch_size = 150,\n",
    "                              regularize = True,\n",
    "                              regLambda = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▏                                                                          | 810/50000 [00:00<00:06, 7952.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix is full rank\n",
      "Regularization - Gradient Descent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 50000/50000 [00:03<00:00, 13446.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35.50937587 11.08940232  7.29592748  4.34045552 -4.01296937  1.95870087\n",
      "  0.22722463 -0.14746242  7.29396228]\n",
      "11.104586195591038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "regression_concrete.trainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-2.3</th>\n",
       "      <th>0.568</th>\n",
       "      <th>4.78</th>\n",
       "      <th>3.99</th>\n",
       "      <th>3.17</th>\n",
       "      <th>0.125</th>\n",
       "      <th>0.11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.3</td>\n",
       "      <td>0.568</td>\n",
       "      <td>4.78</td>\n",
       "      <td>3.99</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.3</td>\n",
       "      <td>0.568</td>\n",
       "      <td>4.78</td>\n",
       "      <td>3.99</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.3</td>\n",
       "      <td>0.568</td>\n",
       "      <td>4.78</td>\n",
       "      <td>3.99</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.3</td>\n",
       "      <td>0.568</td>\n",
       "      <td>4.78</td>\n",
       "      <td>3.99</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.225</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.3</td>\n",
       "      <td>0.568</td>\n",
       "      <td>4.78</td>\n",
       "      <td>3.99</td>\n",
       "      <td>3.17</td>\n",
       "      <td>0.250</td>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   -2.3  0.568  4.78  3.99  3.17  0.125  0.11\n",
       "0  -2.3  0.568  4.78  3.99  3.17  0.150  0.27\n",
       "1  -2.3  0.568  4.78  3.99  3.17  0.175  0.47\n",
       "2  -2.3  0.568  4.78  3.99  3.17  0.200  0.78\n",
       "3  -2.3  0.568  4.78  3.99  3.17  0.225  1.18\n",
       "4  -2.3  0.568  4.78  3.99  3.17  0.250  1.82"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yacht = pd.read_csv(\"yachtData.csv\")\n",
    "yacht.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_yacht  = LinearRegression(yacht.values[:, 0:-1], yacht.values[:,-1], \n",
    "                              learningrate = 0.000001,\n",
    "                              tolerance = 0.0000001,\n",
    "                              gd = True,\n",
    "                              error = 'rmse',\n",
    "                              stochastic = False,\n",
    "                              batch_size = 150,\n",
    "                              regularize = True,\n",
    "                              regLambda = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▋                                                                       | 1822/50000 [00:00<00:02, 17562.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix is full rank\n",
      "Regularization - Gradient Descent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████████████████████████████████████████▊                               | 28616/50000 [00:01<00:00, 24865.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No further imporvement\n",
      "[10.67000694  0.53987566 -0.64163478 -0.11601129 -0.06174425 -0.22182343\n",
      " 12.8031058 ]\n",
      "9.025975982731234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "regression_yacht.trainModel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
